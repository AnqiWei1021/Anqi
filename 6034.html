<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>In the Digital Humanities Era of AI: Reflection From Global Resources to Risks of Large Models</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <!-- 顶部标题区 -->
    <header class="site-header">
        <h1 class="title">Anqi Wei, MA – Digital Arts & Humanities</h1>
        <p class="subtitle">A student & creative thinker</p>

        <nav class="navbar">
            <a href="index.html">Home</a>
            <a href="portfolio.html">Portfolio</a>

            <div class="dropdown">
                <span class="dropbtn">Writing ▼</span>
                <div class="dropdown-content">
                    <a href="6034.html">DH6034 — Humanities and New Technologies: Tools and Methodologies</a>
                    <a href="6033.html">DH6033 – Conceptual Introduction to DAH</a>
                    <a href="6013.html">DH6013 – Graduate Research and Generic Skills</a>
                    <a href="6032.html">DH6032 – Communities of Practice in Digital Scholarship</a>
                    <a href="6006.html">DH6006 – Teaching and Learning in DH</a>
                    <a href="6012.html">DH6012 – Publishing & Editing</a>
                </div>
            </div>
        </nav>
    </header>
 <!-- 写作内容区域 -->
       <main class="writing-container">

    <!-- ===== 顶部信息（左对齐） ===== -->
   <div class="my-info">
    <p>Anqi Wei 125101394</p>
    <p>DH6034: Humanities and New Technologies: Tools and Methodologies</p>
    <p>Lecturer: Dr. Shawn Day</p>
    <p>Last Updated: 2026-02-26</p>
</div>

           
           <!-- ===== 居中大标题 ===== -->
    <h1 class="writing-title center-title">
        Community Engaged Crowdsourcing: My Experience with Irish Weather Rescue
    </h1>

           
 
 <!-- ===== 正文开始 ===== -->
    <div class="writing-content">

        <h2>Introduction</h2>
        <p>In this assignment, I participated in the project called Irish Weather Rescue on the platform of Zooniverse. This project mainly focuses on transcribing the handwritten rainfall records of the 20th century to support the reconstruction of Ireland’s historical climate data. During my participation, I selected the 20S Dublin Data 2 section and completed the transcription of daily rainfall records from the periods 1920-1923 and 1926-1929. A total of 99 months of rainfall information has been entered. This article will reflect on the specific process of my participation in the Irish Weather Rescue crowdsourcing project, including the importance of my contribution to the project goals, and the wider value of crowdsourcing practice to academic research and the social level, and will pay attention to the ethical issues involved. Finally, I will reflect on my personal gains from this crowdsourcing practice and explore how this experience influences my future academic work.</p>
       
        <figure class="writing-figure">
    <img src="images/0zooniverse.jpg"
     alt="zooniverse.jpg"
     style="width:100%; max-width:900px; display:block; margin:auto;">
    <figcaption>(Figure 1. Project selection interface on Zooniverse.)</figcaption>
</figure>

        <figure class="writing-figure">
    <img src="images/1IrishWeatherReduce.jpg"
     alt="IrishWeatherReduce.jpg"
     style="width:100%; max-width:900px; display:block; margin:auto;">
    <figcaption>(Figure 2. Irish Weather Rescue transcription interface.)</figcaption>
</figure>
        
        <figure class="writing-figure">
    <img src="images/2 20sDUBLINDATA2.jpg"
     alt="2 20sDUBLINDATA2.jpg"
     style="width:100%; max-width:900px; display:block; margin:auto;">
    <figcaption>(Figure 3. Monthly rainfall data entry interface.)</figcaption>
</figure>
<hr class="section-divider">
        
        <h2>Process</h2>     
<p>
Before engaging in the crowdsourcing task, I first explored many different types of crowdsourcing projects on the Zooniverse platform, and finally chose Irish Weather Rescue. The reason why I chose this project is, on the one hand, that I have always been curious about Ireland’s rainy climate characteristics, and I hoped that through this practice better understand this phenomenon; on the other hand, the project takes the transcription of handwritten rainfall data from the 20th century as its main task, which strongly related to typical digital humanities archival work. After completing the account sign-out and carefully reading the tutorial and field guide provided by the project, I entered the 20S Dublin Data 2 Section and officially began to transcribe historical rainfall records month by month.
</p>
<p>
During the transcription process, I mainly followed the platform’s instructions to enter the monthly daily rainfall data. To be specific, first, I need to identify the handwritten value of the corresponding month, and then accurately enter it into the system table; for dates marked in blank or with dashes, leave them blank according to the specifications. At the same time, I also need to fill in the per month total rainfall, and judge whether there are any changes or unsual situation on the page, if necessary, make notes in the additional remarks column. Through this process, I have gradually completed the systematic transcription work for several years.
</p>
<p>Several practical difficulties emerged during the transcription process. For instance, in some of the records from 1923, 1928, and 1929, there are obvious change traces, which require me to carefully judge through enlarge image and repeated comparisons. In the case of incomplete determination, I choose to leave it blank or add additional remarks according to the project guide to reduce the risk of misrecording. This experience not only improved my patience and attention to historical data but also made me realize that in crowdsourced transcription, the quality of judgment of individual volunteers will directly affect the data’s reliability. Therefore, the participants themselves play an important role in the whole data production chain (Hawkins et al., 2019).</p>

        <figure class="writing-figure">
    <img src="images/20 8月22和24有涂改.jpg"
     alt="20 8月22和24有涂改.jpg"
     style="width:100%; max-width:900px; display:block; margin:auto;">
    <figcaption>(Figure 4. Example of corrected handwritten rainfall entry for September 1928.)</figcaption>
</figure>
        <hr class="section-divider">
        
<h2>Implications of My Contribution</h2>
<p>In this task, I have completed a total of 99 months of rainfall data entry. Although this represents only a small portion of the overall project, my participation made me realize that each individual transcription contributes to the integrity of Ireland’s historical climate database, which reflects many small scales contribution will support large-scale digital humanities datasets (Ridge, 2014). When dealing with handwritten meteorological records from a hundred years ago, I clearly felt the importance of manual judgment. These records were written day by day, and there are many places that have traces of post-correction and modification. Therefore, when identifying numbers with traces, it is often necessary to repeatedly enlarge the image and make comparisons before a relatively reliable judgment can be made. Through this process, it made me understand that at present, such detailed recognition work still cannot be completely relied on machines or automated tools, because some ambiguous situations still require human experience for judgment (Ridge, 2014). In this sense, although the crowdsourced work of volunteers may seem repetitive or routine, they play an important role in the product data processing. </p>

        <figure class="writing-figure">
    <img src="images/31 1923年数据标记更改.jpg"
     alt="31 1923年数据标记更改.jpg"
     style="width:100%; max-width:900px; display:block; margin:auto;">
    <figcaption>(Figure 5. Corrected monthly rainfall totals for 1923.)</figcaption>
</figure>
        
<p>From another perspective, the value of crowdsourcing projects does not only lie in organizing or transcribing the historical data itself, but it also slowly changes the participatory ways of digital humanities research. In recent years, many crowdsourcing platforms have occurred on the Internet, such as Zooniverse, Smithsonian Digital Volunteers, and iNaturalist… they encourage the public to directly participate in the processing of various cultural and historical materials. In this environment, many non-professional participants can directly enter the process of recording historical or interesting data, which no longer relies on a traditional research institution or professional guidance. As Ridge (2014) notes, <em>“Asking members of the public to help with tasks can be hugely productive.”</em> I found this openness particularly meaningful. On the one hand, participants can flexibly choose a project based on their personal interests, and in practice, gradually understand the meaning of historical data; on the other hand, these platforms also make it easier for the general public to participate in academic-related work. Personally, this experience itself is attractive and interesting, because it allowed me for the first time to truly work hands-on with original meteorological records from over a hundred years ago. This participatory model, to a certain extent, promotes digital humanities development. It not only expands the human resource base for data processing, but it also narrows the distance between the general public and history; it makes some remote or closed historical records gradually transform into digital resources that can be jointly participated in and understood.</p>
<p>However, in the actual processing, I also gradually realize that the crowdsourcing model itself is not limited. First, different volunteers have different standards when they read handwritten materials, and judging fuzzy data may not be completely consistent. For example, a handwritten “3” might reasonably be read by another volunteer as an “8” or even a “4”. When I was dealing with some records from 1923, 1928, and 1929, I encountered many situations that required subjective judgment. If subjective judgment was wrong, it could indeed influence the quality of subsequent data. And then, such projects mainly rely on the free participation of volunteers (Causer and Wallace, 2012), which also made me rethink the relationship between the voluntary contribution and digital labour. Although many participants are motivated primarily by interest and learning, in the long term, how the platform controls quality and participation incentives is still a question. Therefore, I think it is also necessary to maintain a certain critical understanding of its potential risks while affirming the positive promoting effect of crowdsourcing on climate research and digital humanities.</p>

        <figure class="writing-figure">
    <img src="images/25 1920年1-11月未记录.jpg"
     alt="25 1920年1-11月未记录.jpg"
     style="width:100%; max-width:900px; display:block; margin:auto;">
    <figcaption>(Figure 6. Example of unclear handwritten rainfall entries for 1 and 23 in the 1920 record.)</figcaption>
</figure>
        
        
        <hr class="section-divider">
        
<h2>What I learned</h2>
<p>Through the joint crowdsourcing project of Irish Weather Rescue, I have become more familiar with the detailed operation, and also better understood how crowdsourcing works. The most direct changes are reflected in the specific operation level. When I first began to enter these handwritten rainfall records, I did not have much confidence, especially when encountering pages where the handwriting is blurred, modified, or the table format is not clear, which often needed me to repeatedly check to make a judgment. However, with the number of transcriptions increasing, I began to use some strategies, such as amplification, line-by-line comparison, and repeated checking of data, and the whole entry processing became smoother. This experience trains my patience to observe detail in large extent. Meanwhile, I gradually realized that the foundation data entry plays an important role in the digital humanities project.</p>
<p>In the specific operation processing, one experience has a particularly obvious impact on me. While working on records from around 1921, I noticed that the original table format is not clear, and then some of the data itself is a little fuzzy. To address this, I tried to visually divide the auxiliary line by myself and partitioned the data of the three months into groups, which helped me enter the numbers more quickly and accurately. At first, I did this just to make it easier for me to see the data clearly. But as the transcription progressed, I realized that these historical records themselves were not as "standard" as I had imagined; in some cases, the readability of the data still requires manual organization, and it cannot completely rely on the interface itself.</p>
<p>As I continued transcribing, I also encountered another situation deep impact me. In the case of the 1920 rainfall records, the Zooniverse platform provides me with two different versions of the page: in the first version, only the December rainfall data is recorded, January to November are blank; and in the second version provides a complete record for the whole year. What makes me even more confused is that two versions of the data for December are not coincide. This difference made me not sure how to understand it, and also made me think about why two different data versions appear in the same year; moreover, there are obvious differences between the two. Combine my judgment with crowdsourcing, some participants perhaps face is not always a fully cleaned and unified data source; historical archives may have different versions in the process of preservation and digitization. </p>
<p>Baes on these specific experiences, my understanding on crowdsourcing transcription work have some change. Human participants in crowdsourcing work are not just mechanical input data; rather, they continue to participate in identifying, cleaning, and judging data structures, especially when they deal with some fuzzy pages, they play an important role in the data transcription (Lintott et al., 2008). From this perspective, crowdsourcing is not only an efficiency tool, but also it is a collaborative way of knowledge production. However, this model is also risky: different volunteers have different standard to fuzzy numbers, and this difference may affect the data reliability. In general, this experience made me better understand the potential and limitations of crowdsourcing in digital humanities research.</p>
<hr class="section-divider">
    
<h2>Conclusion/Application to my own work</h2>
<p>In the practice of Irish Weather Rescue, I begin to specifically think about how these transcription experiences influence my study and future research. For my current study in digital humanities, I no longer understand digital process from theoretical level; now I will pay more attention to the process of sorting, judging, and quality control of the data before entering the analysis stage. In the past, when I was exposed to digital humanities in the classroom, I always focused on digital tools and data outcomes, but this practice made me realize that the early stage of data production also has decisive significance. In the future, when processing historical data or materials, I will actively pay more attention to the reliability of data sources and some potential human errors.</p>
<p>For a long-term perspective, this experience also made me start to seriously think, in the future, how to use a practical crowdsourcing method in my own research project. For instance, when dealing with some basic statistical tasks, I can also consider using crowdsourcing to publish some online tasks and recruit some volunteers to participate in completing the data processing. In the process, I will take extra notice on operation guide to reduce some human error. Meanwhile, I also consider whether it was necessary to add a manual review or data comparison to improve data reliability.</p>
<p>At the same time, I began to gradually realize that digital humanities not just use technology to process data, but also like establishing a connection between historical materials, digital platforms, and the general public. Projects like Irish Weather Rescue made me directly see how digital humanities through digital platforms connected public and academic data production. Based on this experience, I will pay more attention to the interactive ways between the platform, data, and the public. In summary, this practical experience changed my original imagine on the way of digital humanities, it is more open, inclusive, and diverse.</p>


    
    <hr class="section-divider">

        <h2>References</h2>
    <ul class="reference-list">
        
<li>
    Causer, T. and Wallace, V. (2012) ‘Building a volunteer community: results and findings from Transcribe Bentham’, <em> Digital Humanities Quarterly </em>, 6(2).
</li>
<li>
    Hawkins, E., Burt, S., Brohan, P., Lockwood, M., Richardson, H., Roy, M. and Thomas, S. (2019) ‘Hourly weather observations from the Scottish Highlands (1883–1904) rescued by volunteer citizen scientists’, <em>Geoscience Data Journal</em>, 6(2), pp. 160–173.
</li>
<li>
    Lintott, C.J., Schawinski, K., Slosar, A., Land, K., Bamford, S., Thomas, D., Raddick, M.J., Nichol, R.C., Szalay, A., Andreescu, D., Murray, P. and Vandenberg, J. (2008) ‘Galaxy Zoo: morphologies derived from visual inspection of galaxies from the Sloan Digital Sky Survey’, <em>Monthly Notices of the Royal Astronomical Society</em>, 389(3), pp. 1179–1189.
</li>  
<li>
    Ridge, M. (2014) <em> Crowdsourcing Our Cultural Heritage.</em> Farnham: Ashgate.
</li>

        
    </div>
</main>

    
